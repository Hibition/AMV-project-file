{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "from transformers import AdamW, WarmUp, get_linear_schedule_with_warmup\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 4\n",
    "LEARNING_RATE = 2e-5\n",
    "MAX_LEN = 64\n",
    "TRAIN_PATH = \"../reddit-cleanjokes.csv\" \n",
    "MODEL_FOLDER = \".Llama3.2-1B-Fine-Tuning\" \n",
    "\n",
    "model_name_or_path = \"./Llama3.2-1B/\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name_or_path,\n",
    "    # torch_dtype=torch.float16,  \n",
    "    device_map=\"auto\",  \n",
    "    # low_cpu_mem_usage=True  \n",
    ")\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Jokesdataset(Dataset):\n",
    "    '''\n",
    "    This class builds the custom dataset for Dataloader\n",
    "    '''\n",
    "    def __init__(self,data,tokenizer):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.eos_tok = \"<|endoftext|>\"\n",
    "        #Adding JOKE: at the start and EOS TOKEN at end\n",
    "        self.data['Joke'] = self.data['Joke'].apply(lambda x: \"JOKE:\" + str(x) + self.eos_tok)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self,idx):\n",
    "        joke = self.data.iloc[idx,1]\n",
    "    \n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            joke,\n",
    "            None,\n",
    "            add_special_tokens = True,\n",
    "            max_length = MAX_LEN,\n",
    "            pad_to_max_length = True\n",
    "        )\n",
    "\n",
    "        ids = inputs[\"input_ids\"]\n",
    "        mask = inputs[\"attention_mask\"]\n",
    "\n",
    "        return {'ids':torch.tensor(ids,dtype=torch.long),\n",
    "            'mask': torch.tensor(mask,dtype=torch.long),\n",
    "            'target':torch.tensor(ids,dtype=torch.long)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fn(data_loader, model, optimizer, device, scheduler,epoch):\n",
    "    model.train()\n",
    "    for bi, d in tqdm(enumerate(data_loader)):\n",
    "        ids = d[\"ids\"]\n",
    "        mask = d[\"mask\"]\n",
    "        labels = d['target']\n",
    "\n",
    "        ids = ids.to(device, dtype=torch.long)\n",
    "        mask = mask.to(device, dtype=torch.long)\n",
    "        labels = labels.to(device,dtype=torch.long)\n",
    "          \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(\n",
    "            input_ids =ids,\n",
    "            attention_mask=mask,\n",
    "            labels = labels\n",
    "        )\n",
    "\n",
    "        loss, logits = outputs[:2]                        \n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        if scheduler is not None:\n",
    "                scheduler.step()\n",
    "\n",
    "        if (bi+1) % 500 == 0:\n",
    "            print('Epoch [{}/{}], bi[{}/{}], Loss: {:.4f}' \n",
    "                   .format(epoch+1, EPOCHS, bi+1,len(data_loader), loss.item()))\n",
    "\n",
    "device = torch.device(\"cuda\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1 started==============================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "0it [00:00, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "21it [00:13,  1.65it/s]"
     ]
    }
   ],
   "source": [
    "jokes = pd.read_csv(TRAIN_PATH)\n",
    "\n",
    "jokes_dataset = Jokesdataset(jokes,tokenizer)\n",
    "jokes_dataloader = DataLoader(jokes_dataset,\n",
    "                                batch_size=BATCH_SIZE,\n",
    "                                shuffle=True,\n",
    "                                num_workers=4)\n",
    "\n",
    "\n",
    "num_train_steps = int(len(jokes_dataloader) / BATCH_SIZE * EPOCHS)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer,num_warmup_steps=0,num_training_steps=num_train_steps)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f\"EPOCH {epoch+1} started\" + '=' * 30)\n",
    "    train_fn(jokes_dataloader, model, optimizer, device, scheduler,epoch=epoch)\n",
    "        \n",
    "    models_folder = MODEL_FOLDER \n",
    "    if not os.path.exists(models_folder):\n",
    "        os.mkdir(models_folder)\n",
    "    torch.save(model.state_dict(), os.path.join(models_folder, f\"gpt2_joke_generator{epoch}.pt\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
