{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\n",
    "    'gpt2-medium', \n",
    "    trust_remote_code=True, \n",
    "    use_auth_token=None, \n",
    "    local_files_only=False \n",
    ")\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "special_tokens_dict = {'pad_token': '<PAD>'}\n",
    "num_added_toks = tokenizer.add_special_tokens(special_tokens_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 64\n",
    "\n",
    "class Jokesdataset(Dataset):\n",
    "\n",
    "    def __init__(self,data,tokenizer):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.eos_tok = \"<|endoftext|>\"\n",
    "        self.data['Joke'] = self.data['Joke'].apply(lambda x: str(x) + self.eos_tok)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self,idx):\n",
    "        joke = self.data.iloc[idx,1]\n",
    "\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            joke,\n",
    "            None,\n",
    "            add_special_tokens = True,\n",
    "            max_length = MAX_LEN,\n",
    "            pad_to_max_length = True\n",
    "        )\n",
    "\n",
    "        ids = inputs[\"input_ids\"]\n",
    "\n",
    "        return torch.tensor(ids,dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50258\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_path = \"../reddit-cleanjokes.csv\"\n",
    "jokes = pd.read_csv(train_path) \n",
    "\n",
    "dataset = Jokesdataset(jokes,tokenizer)\n",
    "dataloader = DataLoader(dataset,\n",
    "                                batch_size=64,\n",
    "                                shuffle=True)\n",
    "\n",
    "\n",
    "vocab_size = len(tokenizer) \n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "class LSTMTransformerModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, n_heads, num_layers):\n",
    "        super(LSTMTransformerModel, self).__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.tgtEmbedd = nn.Embedding(vocab_size, 2*embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "        \n",
    "        self.transformer_encoder = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(d_model=hidden_dim, nhead=n_heads), \n",
    "            num_layers=2\n",
    "        )\n",
    "        \n",
    "        self.transformer_decoder = nn.TransformerDecoder(\n",
    "            nn.TransformerDecoderLayer(d_model=hidden_dim * 2, nhead=n_heads), \n",
    "            num_layers=num_layers\n",
    "        )\n",
    "        \n",
    "        self.fc_out = nn.Linear(hidden_dim * 2, vocab_size)\n",
    "        \n",
    "    def forward(self, src, tgt):\n",
    "        x = self.embedding(src)\n",
    "        lstm_out, (h_n, c_n) = self.lstm(x)\n",
    "        transformer_encoder_out = self.transformer_encoder(x)\n",
    "        lstm_last_hidden = h_n[-1].unsqueeze(1).repeat(1, src.size(1), 1)  # [batch_size, seq_len, hidden_dim]\n",
    "\n",
    "        combined_input = torch.cat((lstm_last_hidden, transformer_encoder_out), dim=-1)  # [batch_size, seq_len, hidden_dim * 2]\n",
    "        tgt_embedding = self.tgtEmbedd(tgt)\n",
    "\n",
    "        decoder_output = self.transformer_decoder(tgt_embedding, combined_input)\n",
    "\n",
    "        logits = self.fc_out(decoder_output)\n",
    "        output = torch.softmax(logits, dim=-1)\n",
    "        return output\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/scxzc2/anaconda3/envs/pytorch/lib/python3.9/site-packages/torch/nn/modules/transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LSTMTransformerModel(\n",
       "  (embedding): Embedding(50258, 512)\n",
       "  (tgtEmbedd): Embedding(50258, 1024)\n",
       "  (lstm): LSTM(512, 512, batch_first=True)\n",
       "  (transformer_encoder): TransformerEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0-1): 2 x TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (transformer_decoder): TransformerDecoder(\n",
       "    (layers): ModuleList(\n",
       "      (0-5): 6 x TransformerDecoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (multihead_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        (dropout3): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (fc_out): Linear(in_features=512, out_features=50258, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_dim = 512\n",
    "hidden_dim = 512\n",
    "n_heads = 8       \n",
    "num_layers = 6    \n",
    "\n",
    "model = LSTMTransformerModel(vocab_size, embedding_dim, hidden_dim, n_heads, num_layers)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "\n",
    "def train(model, dataloader, learning_rate, num_epochs):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_loss = 0.0  \n",
    "        loss_num = 0\n",
    "        for sentence in tqdm(dataloader):\n",
    "            \n",
    "            for t in range(MAX_LEN - 3):\n",
    "                input_t = sentence[:, :t+1].to(device) \n",
    "                target_t = sentence[:, t+1].to(device)  \n",
    "                \n",
    "                # print(sentence[0])\n",
    "                # print(input_t[0])\n",
    "                # print(target_t[0])\n",
    "\n",
    "                output_t = model(input_t, target_t)\n",
    "\n",
    "                loss = criterion(output_t[:, -1], target_t)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                epoch_loss += loss.item()\n",
    "                loss_num = loss_num + 1\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {epoch_loss/loss_num:.4f}\")\n",
    "\n",
    "\n",
    "learning_rate = 0.001\n",
    "num_epochs = 5\n",
    "\n",
    "train(model, dataloader, learning_rate, num_epochs)\n",
    "torch.save(model.state_dict(), 'lstm-transformer.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'If life gives you melons<PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD>'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def choose_from_top(probs, n=5):\n",
    "    ind = np.argpartition(probs, -n)[-n:]\n",
    "    top_prob = probs[ind]\n",
    "    top_prob = top_prob / np.sum(top_prob) # Normalize\n",
    "    choice = np.random.choice(n, 1, p = top_prob)\n",
    "    token_id = ind[choice][0]\n",
    "    return int(token_id)\n",
    "\n",
    "def predict(input_text, length_of_joke,number_of_jokes):\n",
    "    joke_num = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for joke_idx in range(number_of_jokes):\n",
    "        \n",
    "            joke_finished = False\n",
    "\n",
    "            cur_ids = torch.tensor(tokenizer.encode(input_text)).unsqueeze(0).to(device)\n",
    "\n",
    "            for i in range(length_of_joke):\n",
    "                outputs = model(cur_ids, cur_ids)\n",
    "                output = outputs[0][-1].argmax()\n",
    "                \n",
    "                cur_ids = torch.cat([cur_ids, torch.ones((1,1)).long().to(device) * output], dim = 1)\n",
    "                if output in tokenizer.encode('<|endoftext|>'):\n",
    "                    joke_finished = True\n",
    "                    break\n",
    "                \n",
    "                word = tokenizer.decode(output)\n",
    "                \n",
    "                input_text = input_text + word\n",
    "\n",
    "                \n",
    "            \n",
    "            # if joke_finished:\n",
    "                \n",
    "        joke_num = joke_num + 1\n",
    "                \n",
    "        output_list = list(cur_ids.squeeze().to('cpu').numpy())\n",
    "        output_text = tokenizer.decode(output_list)\n",
    "\n",
    "        return output_text\n",
    "\n",
    "# Start Predicting\n",
    "input_text = \"If life gives you melons\"\n",
    "\n",
    "predict(input_text, 64, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = \"How did your mom know\"\n",
    "\n",
    "predict(input_text, 64, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Jokesdataset(Dataset):\n",
    "    '''\n",
    "    This class builds the custom dataset for Dataloader\n",
    "    '''\n",
    "    def __init__(self,data,tokenizer):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.eos_tok = \"<|endoftext|>\"\n",
    "        #Adding JOKE: at the start and EOS TOKEN at end\n",
    "        self.data['Joke'] = self.data['Joke'].apply(lambda x: \"JOKE:\" + str(x) + self.eos_tok)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self,idx):\n",
    "        joke = self.data.iloc[idx,1]\n",
    "    \n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            joke,\n",
    "            None,\n",
    "            add_special_tokens = True,\n",
    "            max_length = 64,\n",
    "            pad_to_max_length = True\n",
    "        )\n",
    "\n",
    "        ids = inputs[\"input_ids\"]\n",
    "        mask = inputs[\"attention_mask\"]\n",
    "\n",
    "        return {'ids':torch.tensor(ids,dtype=torch.long),\n",
    "            'mask': torch.tensor(mask,dtype=torch.long),\n",
    "            'target':torch.tensor(ids,dtype=torch.long)}\n",
    "        \n",
    "    def random_split_joke(self, idx):\n",
    "        joke = joke = self.data.iloc[idx,1]\n",
    "        words = joke.split()\n",
    "        split_ratio = np.random.uniform(0.3, 0.7)  # 随机比例\n",
    "        split_index = int(len(words) * split_ratio)\n",
    "        return \" \".join(words[:split_index]), joke\n",
    "\n",
    "# jokes = pd.read_csv(\"/home/scxzc2/project/jokGen/reddit-cleanjokes.csv\") #add the path to your Dataset in config File\n",
    "jokes = pd.read_csv(\"/home/scxzc2/project/jokGen/val.csv\") #add the path to your Dataset in config File\n",
    "\n",
    "test_dataset = Jokesdataset(jokes,tokenizer)\n",
    "test_dataloader = DataLoader(test_dataset,\n",
    "                                batch_size=1,\n",
    "                                shuffle=True,\n",
    "                                num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 53.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(NSFW) A blind man walks\n",
      "(NSFW) A blind man walks into a bar.... So Nickelback walks into a bar, and there is no punchline, because ruining music isn't funny.<|endoftext|>\n",
      "How many Germans does it take\n",
      "How many Germans does it take to change a lightbulb? Just one, but it takes 5 episodes.<|endoftext|>\n",
      "Do flashers have dreams where they leave the house\n",
      "Do flashers have dreams where they leave the house? Intruder window<|endoftext|>\n",
      "What does the Interstellar soundtrack and a porno film\n",
      "What does the Interstellar soundtrack and a porno film? A cat-naaaaa<|endoftext|>\n",
      "The ducks in Cern What does the ducks\n",
      "The ducks in Cern What does the ducks haveman and Eyore had a baby. The baby's name? Supereyore<|endoftext|>\n",
      "What would you have if all autos in\n",
      "What would you have if all autos in the middle? \"Is that you coffin?\"<|endoftext|>\n",
      "Kennedy put a man on\n",
      "Kennedy put a man on my door and asked for a small donation towards the local swimming pool. I gave him a glass of water.<|endoftext|>\n",
      "Girl likes 'boys with accents <333' on Facebook. I charge at\n",
      "Girl likes 'boys with accents <333' on Facebook. I charge at Disney with the kids this week...<|endoftext|>\n",
      "Why do people like amputee porn?\n",
      "Why do people like amputee porn? Because they have their own scales!<|endoftext|>\n",
      "what nation do most\n",
      "what nation do most people live in? Denial. Myself included.<|endoftext|>\n",
      "AVG BLEU score: 0.49808087346902263\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "\n",
    "jokeId = [53, 67, 183, 1345, 489, 982, 322, 283, 432, 763]\n",
    "\n",
    "num = 0\n",
    "total_belu = 0\n",
    "for i in tqdm(range(10)):\n",
    "    input, joke = test_dataset.random_split_joke(jokeId[i])  \n",
    "    \n",
    "    input = input.replace(\"JOKE:\", \"\")\n",
    "    \n",
    "    outputs = []\n",
    "    for j in range(1):\n",
    "        output = predict(input, 64, 1)\n",
    "        outputs.append(output)\n",
    "    \n",
    "    print(input)\n",
    "    print(outputs[0])\n",
    "        \n",
    "    references = [[joke] for _ in range(len(outputs))]\n",
    "    # print(references[0])\n",
    "    \n",
    "    bleu_score = corpus_bleu(references, outputs)\n",
    "    \n",
    "    total_belu += bleu_score\n",
    "    num = num + 1\n",
    "    # print(bleu_score)\n",
    "\n",
    "avg_score = total_belu / num\n",
    "print(f\"AVG BLEU score: {avg_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 8])\n",
      "What did the bartender say to the jumper cables? You better not try to start anything.<|endoftext|>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "def calculate_perplexity(model, tokenizer, text):\n",
    "    # 编码输入文本\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "    input_ids = inputs[\"input_ids\"].to(device)\n",
    "    \n",
    "    # 获取模型输出\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, labels=input_ids)\n",
    "        loss = outputs.loss  # 模型的交叉熵损失\n",
    "\n",
    "    # 计算 Perplexity\n",
    "    perplexity = torch.exp(loss)\n",
    "    return perplexity.item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 5195,   836,   470,  5519,  3774, 23235,    30,  4362,   484,   787,\n",
      "           510,  2279,     0]], device='cuda:0')\n",
      "Perplexity: 37.42204666137695\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from torch import nn\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "def calculate_perplexity(model, tokenizer, text):\n",
    "    # 编码输入文本\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "    sentence = inputs[\"input_ids\"].to(device)\n",
    "    \n",
    "    print(sentence)\n",
    "    \n",
    "    # 获取模型输出\n",
    "    with torch.no_grad():\n",
    "        for t in range(len(sentence)):\n",
    "                input_t = sentence[:, :t+1].to(device) \n",
    "                target_t = sentence[:, t+1].to(device)  \n",
    "\n",
    "                output_t = model(input_t)\n",
    "\n",
    "                loss = criterion(output_t[:, -1], target_t)\n",
    "\n",
    "    # 计算 Perplexity\n",
    "    perplexity = torch.exp(loss)\n",
    "    return perplexity.item()\n",
    "\n",
    "\n",
    "example_text = \"Why don't scientists trust atoms? Because they make up everything!\"\n",
    "# example_text = \"If life gives you melons, you might have dyslexia.\"\n",
    "\n",
    "# 计算 Perplexity\n",
    "ppl = calculate_perplexity(model, tokenizer, example_text)\n",
    "print(f\"Perplexity: {ppl}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
