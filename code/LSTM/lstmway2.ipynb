{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\n",
    "    'gpt2-medium', \n",
    "    trust_remote_code=True, \n",
    "    use_auth_token=None, \n",
    "    local_files_only=False \n",
    ")\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "special_tokens_dict = {'pad_token': '<PAD>'}\n",
    "num_added_toks = tokenizer.add_special_tokens(special_tokens_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 64\n",
    "\n",
    "class Jokesdataset(Dataset):\n",
    "\n",
    "    def __init__(self,data,tokenizer):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.eos_tok = \"<|endoftext|>\"\n",
    "        self.data['Joke'] = self.data['Joke'].apply(lambda x: str(x) + self.eos_tok)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self,idx):\n",
    "        joke = self.data.iloc[idx,1]\n",
    "\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            joke,\n",
    "            None,\n",
    "            add_special_tokens = True,\n",
    "            max_length = MAX_LEN,\n",
    "            pad_to_max_length = True\n",
    "        )\n",
    "\n",
    "        ids = inputs[\"input_ids\"]\n",
    "\n",
    "        return torch.tensor(ids,dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim):\n",
    "\n",
    "        super(LanguageModel, self).__init__()\n",
    "        self.embeddedLayer = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim,  batch_first=True)\n",
    "        \n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "        \n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embeddedLayer(x)\n",
    "        lstm_out, (hn, cn) = self.lstm(x)\n",
    "        output = self.fc(lstm_out)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = \"../reddit-cleanjokes.csv\"\n",
    "jokes = pd.read_csv(train_path) \n",
    "\n",
    "dataset = Jokesdataset(jokes,tokenizer)\n",
    "dataloader = DataLoader(dataset,\n",
    "                                batch_size=32,\n",
    "                                shuffle=True)\n",
    "\n",
    "vocab_size = len(tokenizer)\n",
    "embedding_dim = 512\n",
    "hidden_dim = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50258"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LanguageModel(\n",
       "  (embeddedLayer): Embedding(50258, 512)\n",
       "  (lstm): LSTM(512, 1024, batch_first=True)\n",
       "  (fc): Linear(in_features=1024, out_features=50258, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LanguageModel(vocab_size, embedding_dim,hidden_dim)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/51 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/scxzc2/anaconda3/envs/pytorch/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2673: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 51/51 [01:46<00:00,  2.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 2.1399\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 51/51 [01:48<00:00,  2.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10, Loss: 1.2591\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 51/51 [01:48<00:00,  2.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10, Loss: 0.5741\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 51/51 [01:48<00:00,  2.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10, Loss: 0.3035\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 51/51 [01:48<00:00,  2.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10, Loss: 0.2207\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 51/51 [01:48<00:00,  2.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10, Loss: 0.1905\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 51/51 [01:49<00:00,  2.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10, Loss: 0.1718\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 51/51 [01:49<00:00,  2.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10, Loss: 0.1647\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 51/51 [01:49<00:00,  2.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10, Loss: 0.1539\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 51/51 [01:49<00:00,  2.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10, Loss: 0.1479\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "\n",
    "def train(model, dataloader, learning_rate, num_epochs):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_loss = 0.0  \n",
    "        loss_num = 0\n",
    "        for sentence in tqdm(dataloader):\n",
    "            \n",
    "            for t in range(MAX_LEN - 3):\n",
    "                input_t = sentence[:, :t+1].to(device) \n",
    "                target_t = sentence[:, t+1].to(device)  \n",
    "\n",
    "                output_t = model(input_t)\n",
    "\n",
    "                loss = criterion(output_t[:, -1], target_t)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                epoch_loss += loss.item()\n",
    "                loss_num = loss_num + 1\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {epoch_loss/loss_num:.4f}\")\n",
    "\n",
    "\n",
    "learning_rate = 0.001\n",
    "num_epochs = 10\n",
    "\n",
    "train(model, dataloader, learning_rate, num_epochs)\n",
    "torch.save(model.state_dict(), 'lstmv2-overtrain.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_100373/1941758384.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"lstmv2-overtrain.pth\"))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "model.load_state_dict(torch.load(\"lstmv2-overtrain.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'If life gives you melons, you might have dyslexia.<|endoftext|>'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def choose_from_top(probs, n=5):\n",
    "    ind = np.argpartition(probs, -n)[-n:]\n",
    "    top_prob = probs[ind]\n",
    "    top_prob = top_prob / np.sum(top_prob) # Normalize\n",
    "    choice = np.random.choice(n, 1, p = top_prob)\n",
    "    token_id = ind[choice][0]\n",
    "    return int(token_id)\n",
    "\n",
    "def predict(input_text, length_of_joke,number_of_jokes):\n",
    "    joke_num = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for joke_idx in range(number_of_jokes):\n",
    "        \n",
    "            joke_finished = False\n",
    "\n",
    "            cur_ids = torch.tensor(tokenizer.encode(input_text)).unsqueeze(0).to(device)\n",
    "\n",
    "            for i in range(length_of_joke):\n",
    "                outputs = model(cur_ids)\n",
    "                output = outputs[0][-1].argmax()\n",
    "                \n",
    "                cur_ids = torch.cat([cur_ids, torch.ones((1,1)).long().to(device) * output], dim = 1)\n",
    "                if output in tokenizer.encode('<|endoftext|>'):\n",
    "                    joke_finished = True\n",
    "                    break\n",
    "                \n",
    "                word = tokenizer.decode(output)\n",
    "                \n",
    "                input_text = input_text + word\n",
    "\n",
    "                \n",
    "            \n",
    "            if joke_finished:\n",
    "                \n",
    "                joke_num = joke_num + 1\n",
    "                \n",
    "                output_list = list(cur_ids.squeeze().to('cpu').numpy())\n",
    "                output_text = tokenizer.decode(output_list)\n",
    "\n",
    "                return output_text\n",
    "\n",
    "# Start Predicting\n",
    "input_text = \"If life gives you melons\"\n",
    "\n",
    "predict(input_text, 64, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = \"How did your mom know\"\n",
    "\n",
    "predict(input_text, 64, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Jokesdataset(Dataset):\n",
    "    '''\n",
    "    This class builds the custom dataset for Dataloader\n",
    "    '''\n",
    "    def __init__(self,data,tokenizer):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.eos_tok = \"<|endoftext|>\"\n",
    "        self.data['Joke'] = self.data['Joke'].apply(lambda x: str(x) + self.eos_tok)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self,idx):\n",
    "        joke = self.data.iloc[idx,1]\n",
    "    \n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            joke,\n",
    "            None,\n",
    "            add_special_tokens = True,\n",
    "            max_length = 64,\n",
    "            pad_to_max_length = True\n",
    "        )\n",
    "\n",
    "        ids = inputs[\"input_ids\"]\n",
    "        mask = inputs[\"attention_mask\"]\n",
    "\n",
    "        return {'ids':torch.tensor(ids,dtype=torch.long),\n",
    "            'mask': torch.tensor(mask,dtype=torch.long),\n",
    "            'target':torch.tensor(ids,dtype=torch.long)}\n",
    "        \n",
    "    def random_split_joke(self, idx):\n",
    "        joke = joke = self.data.iloc[idx,1]\n",
    "        words = joke.split()\n",
    "        split_ratio = np.random.uniform(0.3, 0.7)  \n",
    "        split_index = int(len(words) * split_ratio)\n",
    "        return \" \".join(words[:split_index]), joke\n",
    "\n",
    "# jokes = pd.read_csv(\"/home/scxzc2/project/jokGen/reddit-cleanjokes.csv\") #add the path to your Dataset in config File\n",
    "jokes = pd.read_csv(\"/home/scxzc2/project/jokGen/val.csv\") \n",
    "\n",
    "test_dataset = Jokesdataset(jokes,tokenizer)\n",
    "test_dataloader = DataLoader(test_dataset,\n",
    "                                batch_size=1,\n",
    "                                shuffle=True,\n",
    "                                num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 53.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(NSFW) A blind man walks\n",
      "(NSFW) A blind man walks into a bar.... So Nickelback walks into a bar, and there is no punchline, because ruining music isn't funny.<|endoftext|>\n",
      "How many Germans does it take\n",
      "How many Germans does it take to change a lightbulb? Just one, but it takes 5 episodes.<|endoftext|>\n",
      "Do flashers have dreams where they leave the house\n",
      "Do flashers have dreams where they leave the house? Intruder window<|endoftext|>\n",
      "What does the Interstellar soundtrack and a porno film\n",
      "What does the Interstellar soundtrack and a porno film? A cat-naaaaa<|endoftext|>\n",
      "The ducks in Cern What does the ducks\n",
      "The ducks in Cern What does the ducks haveman and Eyore had a baby. The baby's name? Supereyore<|endoftext|>\n",
      "What would you have if all autos in\n",
      "What would you have if all autos in the middle? \"Is that you coffin?\"<|endoftext|>\n",
      "Kennedy put a man on\n",
      "Kennedy put a man on my door and asked for a small donation towards the local swimming pool. I gave him a glass of water.<|endoftext|>\n",
      "Girl likes 'boys with accents <333' on Facebook. I charge at\n",
      "Girl likes 'boys with accents <333' on Facebook. I charge at Disney with the kids this week...<|endoftext|>\n",
      "Why do people like amputee porn?\n",
      "Why do people like amputee porn? Because they have their own scales!<|endoftext|>\n",
      "what nation do most\n",
      "what nation do most people live in? Denial. Myself included.<|endoftext|>\n",
      "AVG BLEU score: 0.49808087346902263\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "\n",
    "jokeId = [53, 67, 183, 1345, 489, 982, 322, 283, 432, 763]\n",
    "\n",
    "num = 0\n",
    "total_belu = 0\n",
    "for i in tqdm(range(10)):\n",
    "    input, joke = test_dataset.random_split_joke(jokeId[i])  \n",
    "    \n",
    "    input = input.replace(\"JOKE:\", \"\")\n",
    "    \n",
    "    outputs = []\n",
    "    for j in range(1):\n",
    "        output = predict(input, 64, 1)\n",
    "        outputs.append(output)\n",
    "    \n",
    "    print(input)\n",
    "    print(outputs[0])\n",
    "        \n",
    "    references = [[joke] for _ in range(len(outputs))]\n",
    "    # print(references[0])\n",
    "    \n",
    "    bleu_score = corpus_bleu(references, outputs)\n",
    "    \n",
    "    total_belu += bleu_score\n",
    "    num = num + 1\n",
    "    # print(bleu_score)\n",
    "\n",
    "avg_score = total_belu / num\n",
    "print(f\"AVG BLEU score: {avg_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 8])\n",
      "What did the bartender say to the jumper cables? You better not try to start anything.<|endoftext|>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 5195,   836,   470,  5519,  3774, 23235,    30,  4362,   484,   787,\n",
      "           510,  2279,     0]], device='cuda:0')\n",
      "Perplexity: 37.42204666137695\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from torch import nn\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "def calculate_perplexity(model, tokenizer, text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "    sentence = inputs[\"input_ids\"].to(device)\n",
    "    \n",
    "    print(sentence)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for t in range(len(sentence)):\n",
    "                input_t = sentence[:, :t+1].to(device) \n",
    "                target_t = sentence[:, t+1].to(device)  \n",
    "\n",
    "                output_t = model(input_t)\n",
    "\n",
    "                loss = criterion(output_t[:, -1], target_t)\n",
    "\n",
    "    perplexity = torch.exp(loss)\n",
    "    return perplexity.item()\n",
    "\n",
    "\n",
    "example_text = \"Why don't scientists trust atoms? Because they make up everything!\"\n",
    "# example_text = \"If life gives you melons, you might have dyslexia.\"\n",
    "\n",
    "ppl = calculate_perplexity(model, tokenizer, example_text)\n",
    "print(f\"Perplexity: {ppl}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
