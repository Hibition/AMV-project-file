{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "wordRange = 4\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        self.listOfWords = self.loadWords()\n",
    "        self.listOfUniqueWords = self.obtainUniqueWords()\n",
    "        self.id2word = {i: w for i, w in enumerate(self.listOfUniqueWords)}\n",
    "        self.word2id = {w: i for i, w in enumerate(self.listOfUniqueWords)}\n",
    "        self.listOfIds = [self.word2id[w] for w in self.listOfWords]\n",
    " \n",
    "    def loadWords(self):\n",
    "        csvData = pd.read_csv('reddit-cleanjokes.csv') \n",
    "        return csvData['Joke'].str.cat(sep=' ').split(' ')\n",
    "\n",
    "    def obtainUniqueWords(self):\n",
    "        wordCounts = Counter(self.listOfWords)\n",
    "        return sorted(wordCounts, key=wordCounts.get, reverse=True)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.listOfIds) - wordRange\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return (torch.tensor(self.listOfIds[index:index+wordRange]).to(device), torch.tensor(self.listOfIds[index+1:index+wordRange+1]).to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim):\n",
    "        super(LanguageModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embeds = self.embedding(x)\n",
    "        lstm_out, _ = self.lstm(embeds)\n",
    "        output = self.fc(lstm_out)\n",
    "        \n",
    "        return output   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataset = MyDataset()\n",
    "\n",
    "vocab_size = len(dataset.listOfUniqueWords)\n",
    " \n",
    "embedding_dim = 128\n",
    "hidden_dim = 256\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LanguageModel(\n",
       "  (embedding): Embedding(6925, 128)\n",
       "  (lstm): LSTM(128, 256, batch_first=True)\n",
       "  (fc): Linear(in_features=256, out_features=6925, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LanguageModel(vocab_size, embedding_dim,hidden_dim)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 748/748 [00:03<00:00, 247.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8, Loss: 4.456206321716309\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 748/748 [00:02<00:00, 314.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/8, Loss: 3.2708797454833984\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 748/748 [00:02<00:00, 315.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/8, Loss: 2.242448568344116\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 748/748 [00:02<00:00, 316.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/8, Loss: 1.651957631111145\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 748/748 [00:02<00:00, 314.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/8, Loss: 1.6460914611816406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 748/748 [00:02<00:00, 313.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/8, Loss: 0.970251739025116\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 748/748 [00:02<00:00, 315.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/8, Loss: 1.1857290267944336\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 748/748 [00:02<00:00, 314.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/8, Loss: 0.41014155745506287\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "learning_rate = 0.001\n",
    "num_epochs = 8\n",
    "\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in tqdm(dataloader):\n",
    "        inputs, targets = batch\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        loss = criterion(outputs.transpose(1,2), targets)  \n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f'Epoch {epoch + 1}/{num_epochs}, Loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "torch.save(model.state_dict(), 'language_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3445212/1907645970.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('language_model.pth'))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('language_model.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义生成文本函数\n",
    "def generate_text(model, start_text, max_words=20):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        input_ids = [dataset.word2id[word] for word in start_text.split()]\n",
    "        for _ in range(max_words):\n",
    "            input_tensor = torch.tensor(input_ids[-4:]).unsqueeze(0)\n",
    "            output = model(input_tensor.to(device))\n",
    "            next_word_id = output.argmax(dim=-1)[:, -1].item()\n",
    "            input_ids.append(next_word_id)\n",
    "    generated_text = ' '.join([dataset.id2word[i] for i in input_ids])\n",
    "    return generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If life gives you melons like to get around Endor? Ewoks I don't have the faintest idea why I passed out Just a short pun\n"
     ]
    }
   ],
   "source": [
    "input_text = \"If life gives you melons\"\n",
    "generated_text = generate_text(model, input_text)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  --------------------------------- EVAL --------------------------------------------\n",
    "\n",
    "# Dataset\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "class Jokesdataset(Dataset):\n",
    "    '''\n",
    "    This class builds the custom dataset for Dataloader\n",
    "    '''\n",
    "    def __init__(self,data):\n",
    "        self.data = data\n",
    "        self.eos_tok = \"<|endoftext|>\"\n",
    "        #Adding JOKE: at the start and EOS TOKEN at end\n",
    "        self.data['Joke'] = self.data['Joke'].apply(lambda x: str(x) + self.eos_tok)\n",
    "\n",
    "        self.listOfWords = self.loadWords()\n",
    "        self.listOfUniqueWords = self.obtainUniqueWords()\n",
    "        self.id2word = {i: w for i, w in enumerate(self.listOfUniqueWords)}\n",
    "        self.word2id = {w: i for i, w in enumerate(self.listOfUniqueWords)}\n",
    "        self.listOfIds = [self.word2id[w] for w in self.listOfWords]\n",
    " \n",
    "    def loadWords(self):\n",
    "        csvData = pd.read_csv('reddit-cleanjokes.csv') \n",
    "        return csvData['Joke'].str.cat(sep=' ').split(' ')\n",
    "\n",
    "    def obtainUniqueWords(self):\n",
    "        wordCounts = Counter(self.listOfWords)\n",
    "        return sorted(wordCounts, key=wordCounts.get, reverse=True)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "        \n",
    "    def random_split_joke(self, idx):\n",
    "        joke = joke = self.data.iloc[idx,1]\n",
    "        words = joke.split()\n",
    "        split_ratio = np.random.uniform(0.3, 0.7)\n",
    "        split_index = int(len(words) * split_ratio)\n",
    "        return \" \".join(words[:split_index]), joke\n",
    "\n",
    "jokes = pd.read_csv(\"/home/scxzc2/project/jokGen/reddit-cleanjokes.csv\") \n",
    "\n",
    "dataset = Jokesdataset(jokes)\n",
    "dataloader = DataLoader(dataset,\n",
    "                                batch_size=1,\n",
    "                                shuffle=True,\n",
    "                                num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 109.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What did the owner of a brownie factory say when his factory caught fire? That he needed to address the situation Math problem: I had 10 chocolate bars and ate\n",
      "What did one snowman say to the other frog? Time's fun when you're having flies. Why did the boy take a ladder to school? He wanted\n",
      "Original physics joke. I'm very proud. I was organizing my desk the other day and I've come to this realization... Currently, this subreddit expecting jokes about soap. I am mildly disappointed. What game\n",
      "What is black, white, and red all over? A Communist Propaganda film from the 1930s. [OC c/o my 9 y.o.] What holds up a bowl's pants? Suspoonders! I don't\n",
      "What did Vincent van Gogh call himself when he joined the Justice League? The Starry Knight Why did the chicken cross the road naked? A: Because\n",
      "I just found out I'm colorblind It came out of the yellow. Ever heard about that movie called Constipation? It never came out. How\n",
      "Always put sunglasses on your tree. Then, you'll get the proper shade. Today I brought a computer back from the dead. I've decided that this\n",
      "What do dwarves use to cut their pizza? Little Caesars What did the fish say when it hit the wall? Dam What does Colonel Mustard's\n",
      "Why can't you run in a camp ground? You can only 'ran'; it's past tents. What kind of soda do dogs drink? Barq's Root Beer. I saw a middle aged\n",
      "I want to die peacefully in my sleep, like my grandfather... Unlike the passengers in his car who were screaming and yelling! http://www.thedailyenglishshow.com/friday-joke/98-how-to-die/ What do you call a\n",
      "AVG BLEU score: 0.5423790946316739\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import nltk\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "\n",
    "jokeId = [13, 7, 183, 1345, 89, 982, 322, 83, 432, 363]\n",
    "\n",
    "num = 0\n",
    "total_belu = 0\n",
    "total_rouge = 0\n",
    "for i in tqdm(range(10)):\n",
    "    input, joke = dataset.random_split_joke(jokeId[i])  \n",
    "    \n",
    "    input = input.replace(\"JOKE:\", \"\")\n",
    "    \n",
    "    outputs = []\n",
    "    for j in range(1):\n",
    "        output = generate_text(model, input, 20)\n",
    "        outputs.append(output)\n",
    "    \n",
    "    # print(input)\n",
    "    print(output)\n",
    "        \n",
    "    references = [[joke] for _ in range(len(outputs))]\n",
    "    \n",
    "    bleu_score = corpus_bleu(references, outputs)\n",
    "\n",
    "    \n",
    "    total_belu += bleu_score\n",
    "    num = num + 1\n",
    "    # print(bleu_score)\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "avg_score = total_belu / num\n",
    "print(f\"AVG BLEU score: {avg_score}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
