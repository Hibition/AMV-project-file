{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have added 1 tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "\n",
    "from transformers import GPT2LMHeadModel\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 4\n",
    "LEARNING_RATE = 2e-5\n",
    "MAX_LEN = 64\n",
    "TRAIN_PATH = \"../shortjokes.csv\" \n",
    "MODEL_FOLDER = \"./trained_models\" \n",
    "Tokenizer = GPT2Tokenizer.from_pretrained(\n",
    "    'gpt2-medium'\n",
    ")\n",
    "\n",
    "def choose_from_top(probs, n=5):\n",
    "    ind = np.argpartition(probs, -n)[-n:]\n",
    "    top_prob = probs[ind]\n",
    "    top_prob = top_prob / np.sum(top_prob)\n",
    "    choice = np.random.choice(n, 1, p = top_prob)\n",
    "    token_id = ind[choice][0]\n",
    "    return int(token_id)\n",
    "\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2-medium')\n",
    "special_tokens_dict = {'pad_token': '<PAD>'}\n",
    "num_added_toks = Tokenizer.add_special_tokens(special_tokens_dict)\n",
    "print('We have added', num_added_toks, 'tokens')\n",
    "model.resize_token_embeddings(len(Tokenizer)) \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50258, 1024)\n",
       "    (wpe): Embedding(1024, 1024)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-23): 24 x GPT2Block(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2SdpaAttention(\n",
       "          (c_attn): Conv1D(nf=3072, nx=1024)\n",
       "          (c_proj): Conv1D(nf=1024, nx=1024)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D(nf=4096, nx=1024)\n",
       "          (c_proj): Conv1D(nf=1024, nx=4096)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1024, out_features=50258, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "models_path = \"./trained_models/gpt2_joke_generator1.pt\"\n",
    "model.load_state_dict(torch.load(models_path))\n",
    "\n",
    "device='cuda'\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def predict(input_text, length_of_joke,number_of_jokes):\n",
    "    joke_num = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for joke_idx in range(number_of_jokes):\n",
    "        \n",
    "            joke_finished = False\n",
    "\n",
    "            cur_ids = torch.tensor(Tokenizer.encode('JOKE:' + input_text)).unsqueeze(0).to(device)\n",
    "\n",
    "            for i in range(length_of_joke):\n",
    "                outputs = model(cur_ids, labels=cur_ids)\n",
    "                loss, logits = outputs[:2]\n",
    "                softmax_logits = torch.softmax(logits[0,-1], dim=0) #Take the first(from only one in this case) batch and the last predicted embedding\n",
    "                if i < 3:\n",
    "                    n = 20\n",
    "                else:\n",
    "                    n = 3\n",
    "                next_token_id = choose_from_top(softmax_logits.to('cpu').numpy(), n=n) #Randomly(from the topN probability distribution) select the next word\n",
    "                cur_ids = torch.cat([cur_ids, torch.ones((1,1)).long().to(device) * next_token_id], dim = 1) # Add the last word to the running sequence\n",
    "\n",
    "                if next_token_id in Tokenizer.encode('<|endoftext|>'):\n",
    "                    joke_finished = True\n",
    "                    break\n",
    "\n",
    "            \n",
    "            if joke_finished:\n",
    "                \n",
    "                joke_num = joke_num + 1\n",
    "                \n",
    "                output_list = list(cur_ids.squeeze().to('cpu').numpy())\n",
    "                output_text = Tokenizer.decode(output_list)\n",
    "\n",
    "                print(output_text+'\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JOKE:If life gives you melons you should eat melons.<|endoftext|>\n",
      "\n",
      "JOKE:If life gives you melons... ...why don't you get melons?<|endoftext|>\n",
      "\n",
      "JOKE:If life gives you melons I have a good idea how I'm gonna make them grow.<|endoftext|>\n",
      "\n",
      "JOKE:If life gives you melons... What do you give to life?<|endoftext|>\n",
      "\n",
      "JOKE:If life gives you melons, then it gives you melons.<|endoftext|>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Start Predicting\n",
    "input_text = \"If life gives you melons\"\n",
    "\n",
    "predict(input_text, 64, 5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
