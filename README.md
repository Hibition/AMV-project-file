# Joke Generation Model

This repository hosts a Joke Generation Model built using PyTorch, designed to generate humorous text with multiple modeling strategies. The project explores different approaches to optimize performance and generate jokes with varying styles and complexity.

## Project Overview

The project contains multiple submodules, each representing a different modeling strategy:

### 1. Optimized LSTM

Implements a customized and optimized LSTM model for sequential joke generation.

- Focuses on capturing context and generating coherent punchlines through effective sequence modeling.

### 2. T5-Based Text-to-Text Strategy

Utilizes the T5 model (Text-to-Text Transfer Transformer) to frame joke generation as a text-to-text task.

- Leverages pre-trained T5 capabilities and fine-tunes it on joke datasets for enhanced humor generation.

### 3. LLaMA Fine-Tuned LLM

Adopts the LLaMA large language model (LLM) for fine-tuned joke generation.

- Incorporates advanced fine-tuning techniques to produce context-aware and humor-rich text.

### 4. GPT-2 Integration

Includes experiments and implementation of GPT-2 for generating jokes with pre-trained language modeling capabilities.

### 5. Joke Classifier

Contains a `jokeClassifier.ipynb` notebook to classify or evaluate jokes based on their humor levels. Import from Hagging face: mohameddhiab/rate-jokes-bert

## Access model

Because the model file size is too big, we create the clond driver link here for someone who want to access that files:

Baidu Cloud：AMV-project-file
Link：https://pan.baidu.com/s/1yVbATnPGuzVt8MR094oaZw?pwd=kdk9 
Code：kdk9


